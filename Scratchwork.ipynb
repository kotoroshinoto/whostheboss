{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from canada_model import *\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SimpleModel.load_from_pickle('./TrainedModels/simple.P', is_path=True)  # type: SimpleModel\n",
    "valid = DataSet.load_from_pickle('./Validation_And_Test_Sets/simple.valid.set.P', is_path=True)  # type: DataSet\n",
    "test = DataSet.load_from_pickle('./Validation_And_Test_Sets/simple.test.set.P', is_path=True)  # type: DataSet\n",
    "valid_pred = model.clf.predict(valid.X)\n",
    "test_pred = model.clf.predict(test.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_model = SimpleModel.load_from_pickle('./TrainedModels/simple.combined.P', is_path=True)  # type: SimpleModel\n",
    "combined_valid = DataSet.load_from_pickle('./Validation_And_Test_Sets/simple.valid.set.combined.P', is_path=True)  # type: DataSet\n",
    "combined_test = DataSet.load_from_pickle('./Validation_And_Test_Sets/simple.test.set.combined.P', is_path=True)  # type: DataSet\n",
    "combined_valid_pred = combined_model.clf.predict(combined_valid.X)\n",
    "combined_test_pred = combined_model.clf.predict(combined_test.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationReporter:\n",
    "    def __init__(self, y, y_pred, classes):\n",
    "        retvals =[]\n",
    "        for retval in metrics.precision_recall_fscore_support(y, y_pred, average='weighted'):\n",
    "            retvals.append(retval)\n",
    "        self.avg_precision = retvals[0]\n",
    "        self.avg_recall = retvals[1]\n",
    "        self.avg_fbeta_score = retvals[2]\n",
    "        self.total = len(y)\n",
    "        retvals = []\n",
    "        for retval in metrics.precision_recall_fscore_support(y, y_pred):\n",
    "            retvals.append(retval)\n",
    "        self.precision = retvals[0]\n",
    "        self.recall = retvals[1]\n",
    "        self.fbeta_score = retvals[2]\n",
    "        self.support = retvals[3]\n",
    "        self.conf_matrix = metrics.confusion_matrix(y, y_pred)\n",
    "        self.cats = classes\n",
    "\n",
    "    def get_report_dataframe(self):\n",
    "        df = pd.DataFrame()\n",
    "        df['Precision'] = pd.Series(data=self.precision).append(pd.Series([self.avg_precision]))\n",
    "        df['Recall'] = pd.Series(data=self.recall).append(pd.Series([self.avg_recall]))\n",
    "        df['F1-Score'] = pd.Series(data=self.fbeta_score).append(pd.Series([self.avg_fbeta_score]))\n",
    "        df['Support'] = pd.Series(data=self.support).append(pd.Series([self.total]))\n",
    "\n",
    "        cats = np.ndarray.copy(self.cats)  # type: np.ndarray\n",
    "        print(len(cats))\n",
    "        cats = np.append(cats, [\"Avg / Total\"])\n",
    "        print(len(cats))\n",
    "        df.index = cats\n",
    "        return df\n",
    "            \n",
    "\n",
    "valid_report = ClassificationReporter(valid.Y, valid_pred, valid.encoder.classes_)\n",
    "test_report = ClassificationReporter(test.Y, test_pred, test.encoder.classes_)\n",
    "combined_valid_report = ClassificationReporter(combined_valid.Y, combined_valid_pred, combined_valid.encoder.classes_)\n",
    "combined_test_report = ClassificationReporter(combined_test.Y, combined_test_pred, combined_test.encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "46\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "print(type(valid.encoder.classes_))\n",
    "df = valid_report.get_report_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Precision    Recall  F1-Score  Support\n",
      "00            1.000000  1.000000  1.000000       82\n",
      "01            0.984962  1.000000  0.992424      131\n",
      "02            1.000000  1.000000  1.000000       41\n",
      "03            0.962963  1.000000  0.981132       26\n",
      "04            0.993421  0.986928  0.990164      153\n",
      "05            1.000000  1.000000  1.000000       58\n",
      "06            1.000000  1.000000  1.000000       76\n",
      "07            0.983607  1.000000  0.991736       60\n",
      "08            1.000000  0.977273  0.988506       44\n",
      "09            0.972222  0.972222  0.972222       36\n",
      "11            1.000000  0.989691  0.994819       97\n",
      "12            0.976608  0.994048  0.985251      168\n",
      "13            0.966667  1.000000  0.983051       29\n",
      "14            0.990385  0.971698  0.980952      106\n",
      "15            0.986111  1.000000  0.993007       71\n",
      "21            1.000000  0.995798  0.997895      238\n",
      "22            1.000000  0.989556  0.994751      383\n",
      "30            1.000000  1.000000  1.000000       34\n",
      "31            0.991071  1.000000  0.995516      111\n",
      "32            0.987730  1.000000  0.993827      161\n",
      "34            1.000000  0.980000  0.989899       50\n",
      "40            0.990826  0.981818  0.986301      110\n",
      "41            0.995349  1.000000  0.997669      214\n",
      "42            1.000000  1.000000  1.000000       65\n",
      "43            0.961538  0.961538  0.961538       26\n",
      "44            0.971429  1.000000  0.985507       34\n",
      "51            1.000000  0.992126  0.996047      127\n",
      "52            1.000000  0.996241  0.998117      266\n",
      "62            1.000000  0.983051  0.991453       59\n",
      "63            0.989130  1.000000  0.994536       91\n",
      "64            1.000000  1.000000  1.000000       88\n",
      "65            0.992908  0.992908  0.992908      141\n",
      "66            1.000000  1.000000  1.000000       24\n",
      "67            1.000000  0.992000  0.995984      125\n",
      "72            1.000000  0.984925  0.992405      398\n",
      "73            0.987805  0.985401  0.986602      411\n",
      "74            1.000000  1.000000  1.000000      124\n",
      "75            0.992126  0.976744  0.984375      129\n",
      "76            0.933884  0.982609  0.957627      115\n",
      "82            0.991453  0.983051  0.987234      118\n",
      "84            0.988506  0.977273  0.982857       88\n",
      "86            0.960000  0.960000  0.960000       50\n",
      "92            0.988166  0.985251  0.986706      339\n",
      "94            0.985702  0.994590  0.990126     1109\n",
      "95            0.993277  0.988294  0.990780      598\n",
      "96            0.986885  0.986885  0.986885      305\n",
      "Avg / Total   0.990793  0.990696  0.990711     7309\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:insight_env]",
   "language": "python",
   "name": "conda-env-insight_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
